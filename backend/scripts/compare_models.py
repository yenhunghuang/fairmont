#!/usr/bin/env python3
"""
æ¨¡å‹æ¯”è¼ƒæ¸¬è©¦è…³æœ¬

æ¯”è¼ƒä¸åŒ Gemini æ¨¡å‹åœ¨ç›¸åŒ Skills é…ç½®ä¸‹çš„æ•ˆèƒ½å·®ç•°ã€‚

ä½¿ç”¨æ–¹å¼ï¼š
    cd backend
    python scripts/compare_models.py --pdf path/to/test.pdf
    python scripts/compare_models.py --mock  # ä½¿ç”¨ mock è³‡æ–™æ¸¬è©¦

ç’°å¢ƒè®Šæ•¸ï¼š
    GEMINI_API_KEY: Gemini API é‡‘é‘°ï¼ˆå¿…è¦ï¼‰
"""

import argparse
import asyncio
import json
import os
import sys
import time
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Optional

# åŠ å…¥ backend åˆ° path
sys.path.insert(0, str(Path(__file__).parent.parent))

import google.generativeai as genai


@dataclass
class ModelResult:
    """å–®ä¸€æ¨¡å‹æ¸¬è©¦çµæœ."""

    model_name: str
    success: bool
    latency_ms: float
    input_tokens: int = 0
    output_tokens: int = 0
    total_tokens: int = 0
    items_count: int = 0
    error: Optional[str] = None
    raw_response: Optional[str] = None
    parsed_items: list = field(default_factory=list)


@dataclass
class ComparisonReport:
    """æ¨¡å‹æ¯”è¼ƒå ±å‘Š."""

    skill_version: str
    test_content_length: int
    results: list[ModelResult] = field(default_factory=list)

    def print_report(self):
        """è¼¸å‡ºæ¯”è¼ƒå ±å‘Š."""
        print("\n" + "=" * 70)
        print("ğŸ“Š æ¨¡å‹æ¯”è¼ƒå ±å‘Š")
        print("=" * 70)
        print(f"Skills ç‰ˆæœ¬: {self.skill_version}")
        print(f"æ¸¬è©¦å…§å®¹é•·åº¦: {self.test_content_length} å­—å…ƒ")
        print("-" * 70)

        # è¡¨é ­
        print(f"{'æ¨¡å‹':<30} {'ç‹€æ…‹':<8} {'å»¶é²':<10} {'Token':<12} {'é …ç›®æ•¸':<8}")
        print("-" * 70)

        for r in self.results:
            status = "âœ… æˆåŠŸ" if r.success else "âŒ å¤±æ•—"
            latency = f"{r.latency_ms:.0f}ms"
            tokens = f"{r.total_tokens:,}" if r.total_tokens else "N/A"
            items = str(r.items_count) if r.success else "-"

            print(f"{r.model_name:<30} {status:<8} {latency:<10} {tokens:<12} {items:<8}")

        print("-" * 70)

        # æˆåŠŸçš„çµæœæ¯”è¼ƒ
        successful = [r for r in self.results if r.success]
        if len(successful) >= 2:
            print("\nğŸ“ˆ æ•ˆèƒ½åˆ†æ:")

            # å»¶é²æ¯”è¼ƒ
            fastest = min(successful, key=lambda x: x.latency_ms)
            slowest = max(successful, key=lambda x: x.latency_ms)
            print(f"  æœ€å¿«: {fastest.model_name} ({fastest.latency_ms:.0f}ms)")
            print(f"  æœ€æ…¢: {slowest.model_name} ({slowest.latency_ms:.0f}ms)")
            print(f"  å·®ç•°: {slowest.latency_ms / fastest.latency_ms:.1f}x")

            # Token æ¯”è¼ƒ
            if all(r.total_tokens > 0 for r in successful):
                min_token = min(successful, key=lambda x: x.total_tokens)
                max_token = max(successful, key=lambda x: x.total_tokens)
                print(f"\n  æœ€å°‘ Token: {min_token.model_name} ({min_token.total_tokens:,})")
                print(f"  æœ€å¤š Token: {max_token.model_name} ({max_token.total_tokens:,})")

            # é …ç›®æ•¸æ¯”è¼ƒ
            item_counts = set(r.items_count for r in successful)
            if len(item_counts) == 1:
                print(f"\n  âœ… æ‰€æœ‰æ¨¡å‹æå–ç›¸åŒæ•¸é‡é …ç›®: {successful[0].items_count}")
            else:
                print(f"\n  âš ï¸ æ¨¡å‹æå–é …ç›®æ•¸ä¸ä¸€è‡´:")
                for r in successful:
                    print(f"     {r.model_name}: {r.items_count} é …")

        print("\n" + "=" * 70)

        # è©³ç´°é …ç›®æ¯”è¼ƒï¼ˆå¦‚æœé …ç›®æ•¸ä¸åŒï¼‰
        if len(successful) >= 2:
            self._compare_items(successful)

    def _compare_items(self, results: list[ModelResult]):
        """æ¯”è¼ƒå„æ¨¡å‹æå–çš„é …ç›®."""
        if not all(r.parsed_items for r in results):
            return

        print("\nğŸ“‹ é …ç›®æ¯”è¼ƒï¼ˆå‰ 3 é …ï¼‰:")
        print("-" * 70)

        for i in range(min(3, max(len(r.parsed_items) for r in results))):
            print(f"\né …ç›® #{i + 1}:")
            for r in results:
                if i < len(r.parsed_items):
                    item = r.parsed_items[i]
                    item_no = item.get("item_no", "N/A")
                    desc = item.get("description", "N/A")[:40]
                    print(f"  [{r.model_name[:20]:<20}] {item_no}: {desc}")


# è¦æ¯”è¼ƒçš„æ¨¡å‹åˆ—è¡¨
MODELS_TO_COMPARE = [
    "gemini-2.0-flash",           # ç©©å®šç‰ˆ Flash
    "gemini-2.0-flash-lite",      # è¼•é‡ç‰ˆ
    "gemini-1.5-flash",           # èˆŠç‰ˆ Flash
    "gemini-1.5-pro",             # Pro ç‰ˆï¼ˆè¼ƒå¼·ä½†è¼ƒè²´ï¼‰
    # "gemini-2.5-flash-preview-05-20",  # é è¦½ç‰ˆï¼ˆå¦‚æœ‰æ¬Šé™ï¼‰
]


# Mock PDF å…§å®¹ï¼ˆç”¨æ–¼ç„¡å¯¦éš› PDF æ™‚æ¸¬è©¦ï¼‰
MOCK_PDF_CONTENT = """
--- Page 1 ---

HABITUS Design Group
FF&E SPECIFICATION BOOK
Project: SOLAIRE BAY TOWER

--- Page 2 ---

ITEM NO.: DLX-100
DESCRIPTION: King Bed
TYPE: Casegoods

Overall Dimensions:
W 2000 x D 2100 x H 1200 mm

QTY: 239 ea

Index:
@ King Deluxe Room Type A
@ King Deluxe Room Type B

--- Page 3 ---

ITEM NO.: DLX-101
DESCRIPTION: Bedside Table
TYPE: Casegoods

Overall Dimensions:
W 600 x D 450 x H 550 mm

QTY: 478 ea

Index:
@ King Deluxe Room Type A
@ King Deluxe Room Type B
@ Queen Standard Room

--- Page 4 ---

ITEM NO.: DLX-102
DESCRIPTION: Round Coffee Table
TYPE: Casegoods

Overall Dimensions:
Dia. 800 x H 450 mm

QTY: 120 ea

Index:
@ Lobby Lounge
@ Executive Lounge

--- Page 5 ---

ITEM NO.: FAB-001 @ DLX-100 King Bed
DESCRIPTION: Fabric for Headboard
TYPE: Fabric

Vendor: Morbern Europe
Pattern: Prodigy PRO 682
Color: Lt Neutral
Width: 137cm
Style: plain

QTY: 500 m
"""


def get_skill_prompt_template() -> tuple[str, str]:
    """è¼‰å…¥ Skills prompt æ¨¡æ¿."""
    from app.services.skill_loader import get_skill_loader

    loader = get_skill_loader()
    skill = loader.load_vendor_or_default("habitus")

    if skill:
        version = skill.version
        template = skill.prompts.parse_specification.user_template
        return version, template
    else:
        # Fallback
        return "default", """
è«‹åˆ†æ PDF å…§å®¹ï¼Œæå– BOQ é …ç›®ã€‚

è¼¸å‡º JSON é™£åˆ—ï¼Œæ¯é …åŒ…å«ï¼š
- item_no: é …ç›®ç·¨è™Ÿ
- description: å“åæè¿°
- dimension: å°ºå¯¸
- qty: æ•¸é‡
- uom: å–®ä½

<document>
{pdf_content}
</document>

è«‹åªè¿”å› JSON æ•¸çµ„ã€‚
"""


async def test_model(
    model_name: str,
    prompt: str,
    api_key: str,
) -> ModelResult:
    """æ¸¬è©¦å–®ä¸€æ¨¡å‹."""

    print(f"  æ¸¬è©¦ {model_name}...", end=" ", flush=True)

    start_time = time.time()

    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel(model_name)

        response = await asyncio.wait_for(
            asyncio.to_thread(model.generate_content, prompt),
            timeout=120,
        )

        latency_ms = (time.time() - start_time) * 1000

        # è§£æ token ä½¿ç”¨é‡
        input_tokens = 0
        output_tokens = 0
        if hasattr(response, "usage_metadata") and response.usage_metadata:
            input_tokens = getattr(response.usage_metadata, "prompt_token_count", 0)
            output_tokens = getattr(response.usage_metadata, "candidates_token_count", 0)

        # è§£æ JSON çµæœ
        raw_text = response.text.strip()

        # æ¸…ç† markdown code block
        if raw_text.startswith("```"):
            lines = raw_text.split("\n")
            raw_text = "\n".join(lines[1:-1] if lines[-1] == "```" else lines[1:])

        try:
            parsed_items = json.loads(raw_text)
            if not isinstance(parsed_items, list):
                parsed_items = []
        except json.JSONDecodeError:
            parsed_items = []

        print(f"âœ… {latency_ms:.0f}ms, {len(parsed_items)} é …ç›®")

        return ModelResult(
            model_name=model_name,
            success=True,
            latency_ms=latency_ms,
            input_tokens=input_tokens,
            output_tokens=output_tokens,
            total_tokens=input_tokens + output_tokens,
            items_count=len(parsed_items),
            raw_response=raw_text[:500],
            parsed_items=parsed_items,
        )

    except asyncio.TimeoutError:
        latency_ms = (time.time() - start_time) * 1000
        print(f"âŒ è¶…æ™‚")
        return ModelResult(
            model_name=model_name,
            success=False,
            latency_ms=latency_ms,
            error="Timeout after 120s",
        )

    except Exception as e:
        latency_ms = (time.time() - start_time) * 1000
        print(f"âŒ {str(e)[:50]}")
        return ModelResult(
            model_name=model_name,
            success=False,
            latency_ms=latency_ms,
            error=str(e),
        )


async def run_comparison(
    pdf_content: str,
    models: list[str],
    api_key: str,
) -> ComparisonReport:
    """åŸ·è¡Œæ¨¡å‹æ¯”è¼ƒ."""

    # è¼‰å…¥ Skills
    skill_version, prompt_template = get_skill_prompt_template()

    # å»ºç«‹å®Œæ•´ prompt
    prompt = prompt_template.format(
        pdf_content=pdf_content,
        categories_instruction="",
    )

    print(f"\nğŸ“‹ ä½¿ç”¨ Skills ç‰ˆæœ¬: {skill_version}")
    print(f"ğŸ“ Prompt é•·åº¦: {len(prompt)} å­—å…ƒ")
    print(f"ğŸ”„ æ¸¬è©¦ {len(models)} å€‹æ¨¡å‹...\n")

    report = ComparisonReport(
        skill_version=skill_version,
        test_content_length=len(pdf_content),
    )

    # ä¾åºæ¸¬è©¦å„æ¨¡å‹ï¼ˆé¿å… rate limitï¼‰
    for model_name in models:
        result = await test_model(model_name, prompt, api_key)
        report.results.append(result)

        # é–“éš”é¿å… rate limit
        if model_name != models[-1]:
            await asyncio.sleep(1)

    return report


def main():
    parser = argparse.ArgumentParser(
        description="æ¯”è¼ƒä¸åŒ Gemini æ¨¡å‹åœ¨ç›¸åŒ Skills ä¸‹çš„æ•ˆèƒ½",
    )
    parser.add_argument(
        "--pdf",
        type=str,
        help="æ¸¬è©¦ç”¨ PDF æª”æ¡ˆè·¯å¾‘",
    )
    parser.add_argument(
        "--mock",
        action="store_true",
        help="ä½¿ç”¨å…§å»º mock è³‡æ–™æ¸¬è©¦",
    )
    parser.add_argument(
        "--models",
        type=str,
        nargs="+",
        default=MODELS_TO_COMPARE,
        help="è¦æ¯”è¼ƒçš„æ¨¡å‹åˆ—è¡¨",
    )

    args = parser.parse_args()

    # æª¢æŸ¥ API Key
    api_key = os.environ.get("GEMINI_API_KEY")
    if not api_key:
        print("âŒ éŒ¯èª¤: è«‹è¨­å®š GEMINI_API_KEY ç’°å¢ƒè®Šæ•¸")
        sys.exit(1)

    # å–å¾—æ¸¬è©¦å…§å®¹
    if args.pdf:
        # å¾ PDF æå–æ–‡å­—
        import fitz  # PyMuPDF

        pdf_path = Path(args.pdf)
        if not pdf_path.exists():
            print(f"âŒ éŒ¯èª¤: æ‰¾ä¸åˆ°æª”æ¡ˆ {pdf_path}")
            sys.exit(1)

        doc = fitz.open(str(pdf_path))
        pdf_content = ""
        for i, page in enumerate(doc):
            pdf_content += f"\n--- Page {i + 1} ---\n"
            pdf_content += page.get_text()
        doc.close()

        print(f"ğŸ“„ è¼‰å…¥ PDF: {pdf_path.name} ({len(pdf_content)} å­—å…ƒ)")

    elif args.mock:
        pdf_content = MOCK_PDF_CONTENT
        print("ğŸ“„ ä½¿ç”¨ Mock è³‡æ–™æ¸¬è©¦")

    else:
        print("âŒ éŒ¯èª¤: è«‹æŒ‡å®š --pdf æˆ– --mock")
        parser.print_help()
        sys.exit(1)

    # åŸ·è¡Œæ¯”è¼ƒ
    report = asyncio.run(run_comparison(
        pdf_content=pdf_content,
        models=args.models,
        api_key=api_key,
    ))

    # è¼¸å‡ºå ±å‘Š
    report.print_report()


if __name__ == "__main__":
    main()
